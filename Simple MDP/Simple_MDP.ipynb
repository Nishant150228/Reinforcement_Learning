{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "35zc4CrNrhnI"
   },
   "source": [
    "#Program 1: Self-Driving Car at an Intersection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NLqkRdn7rh7H"
   },
   "source": [
    "##Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "rBvrz-cVj0p3"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DJzK-5jqj52b"
   },
   "source": [
    "##Environment Class for Self-Driving Car"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "I1ZpNXPpj27r"
   },
   "outputs": [],
   "source": [
    "class SelfDrivingCarEnv:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the environment for the self-driving car.\n",
    "        Define states, actions, initial state, reward structure, and discount factor.\n",
    "        \"\"\"\n",
    "        # Define the possible states\n",
    "        self.states = [\"Green_Moving\", \"Green_Stopped\", \"Red_Moving\", \"Red_Stopped\"]\n",
    "        # Define the possible actions\n",
    "        self.actions = [\"Stop\", \"Drive\"]\n",
    "        # Initialize the current state\n",
    "        self.current_state = \"Red_Stopped\"\n",
    "        # Initialize done flag for the episode\n",
    "        self.done = False\n",
    "        # Set the discount factor\n",
    "        self.gamma = 0.9  # Discount factor\n",
    "        # Define the reward structure based on state-action pairs\n",
    "        self.rewards = {\n",
    "            (\"Green_Moving\", \"Drive\"): 1,         # Reward for driving in a green light\n",
    "            (\"Green_Stopped\", \"Stop\"): 1,         # Reward for stopping at a green light\n",
    "            (\"Green_Stopped\", \"Drive\"): -1,       # Penalty for starting to drive from stopped state (inappropriate action)\n",
    "            (\"Red_Moving\", \"Drive\"): -10,         # Penalty for driving in a red light\n",
    "            (\"Red_Stopped\", \"Stop\"): 0,           # Neutral reward for stopping at a red light\n",
    "            (\"Red_Stopped\", \"Drive\"): -10,        # Penalty for driving while stopped at a red light\n",
    "            (\"Red_Moving\", \"Stop\"): -5,           # Penalty for stopping while in red light\n",
    "        }\n",
    "\n",
    "    # ==========================\n",
    "    # Reset Function\n",
    "    # ==========================\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reset the environment to a random initial state.\n",
    "        Returns the initial state after resetting.\n",
    "        \"\"\"\n",
    "        self.current_state = random.choice(self.states)  # Randomize starting state\n",
    "        self.done = False  # Reset done flag\n",
    "        return self.current_state  # Return the initial state\n",
    "\n",
    "    # ==========================\n",
    "    # Step Function\n",
    "    # ==========================\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Execute the given action in the current state.\n",
    "        Update the state based on the action taken, return the next state,\n",
    "        reward received, and whether the episode is done.\n",
    "        \"\"\"\n",
    "        # Determine the next state and reward based on the current state and action\n",
    "        if self.current_state == \"Green_Moving\":\n",
    "            if action == \"Drive\":\n",
    "                next_state = \"Green_Moving\"\n",
    "                reward = self.rewards[(\"Green_Moving\", \"Drive\")]\n",
    "            else:  # Stop\n",
    "                next_state = \"Green_Stopped\"\n",
    "                reward = self.rewards[(\"Green_Moving\", \"Stop\")]\n",
    "\n",
    "        elif self.current_state == \"Green_Stopped\":\n",
    "            if action == \"Drive\":\n",
    "                next_state = \"Green_Moving\"\n",
    "                reward = self.rewards[(\"Green_Stopped\", \"Drive\")]\n",
    "            else:  # Stop\n",
    "                next_state = \"Green_Stopped\"\n",
    "                reward = self.rewards[(\"Green_Stopped\", \"Stop\")]\n",
    "\n",
    "        elif self.current_state == \"Red_Moving\":\n",
    "            if action == \"Drive\":\n",
    "                next_state = \"Red_Moving\"\n",
    "                reward = self.rewards[(\"Red_Moving\", \"Drive\")]\n",
    "            else:  # Stop\n",
    "                next_state = \"Red_Stopped\"\n",
    "                reward = self.rewards[(\"Red_Moving\", \"Stop\")]\n",
    "\n",
    "        else:  # Red_Stopped\n",
    "            if action == \"Drive\":\n",
    "                next_state = \"Red_Moving\"\n",
    "                reward = self.rewards[(\"Red_Stopped\", \"Drive\")]\n",
    "            else:  # Stop\n",
    "                next_state = \"Red_Stopped\"\n",
    "                reward = self.rewards[(\"Red_Stopped\", \"Stop\")]\n",
    "\n",
    "        # Update the current state\n",
    "        self.current_state = next_state  # Transition to the next state\n",
    "\n",
    "        # Set done condition: if the car is in red and stopped for too long, end the episode\n",
    "        if self.current_state == \"Red_Stopped\" and action == \"Stop\":\n",
    "            self.done = True  # End the episode if stopped at a red light\n",
    "\n",
    "        return next_state, reward, self.done  # Return the results\n",
    "\n",
    "    # ==========================\n",
    "    # Render Function\n",
    "    # ==========================\n",
    "    def render(self):\n",
    "        \"\"\"\n",
    "        Display the current state of the environment.\n",
    "        \"\"\"\n",
    "        print(f\"Current State: {self.current_state}\")  # Output the current state of the car\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4EjraqQrkSVI"
   },
   "source": [
    "## Simulation Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M4ZLWXdOkPhm",
    "outputId": "99550f6c-7fc5-4ca8-d21b-1fa58146ae1e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current State: Red_Stopped\n",
      "Action Taken: Stop, Reward: -5, Next State: Red_Stopped\n",
      "Episode finished.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Create an instance of the SelfDrivingCar environment\n",
    "    env = SelfDrivingCarEnv()\n",
    "    # Reset the environment to get the initial state\n",
    "    state = env.reset()\n",
    "    done = False  # Initialize done flag\n",
    "\n",
    "    # Run the simulation\n",
    "    while not done:\n",
    "        # Randomly choose an action for simplicity\n",
    "        action = random.choice(env.actions)  # Select a random action from available actions\n",
    "        # Execute the action and observe the next state and reward\n",
    "        next_state, reward, done = env.step(action)  # Take a step in the environment\n",
    "        # Render the current state of the environment\n",
    "        env.render()  # Show the current state\n",
    "        # Print the action taken, reward received, and next state\n",
    "        print(f\"Action Taken: {action}, Reward: {reward}, Next State: {next_state}\")  # Output the action, reward, and next state\n",
    "\n",
    "    print(\"Episode finished.\")  # Indicate that the episode has ended"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iQy5D7LoqSgx"
   },
   "source": [
    "# Program 2: Robot Navigation in a Grid World"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "haM7nEegqX2I"
   },
   "source": [
    "## Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "7SkUBlvuotkK"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OFBZZOHNqbyt"
   },
   "source": [
    "## Environment Class for Grid World"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "H3cwyiGRqbPk"
   },
   "outputs": [],
   "source": [
    "class GridWorldEnv:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the environment for the grid world.\n",
    "        Define the grid size, state space, action space, initial state,\n",
    "        goal state, obstacles, and discount factor.\n",
    "        \"\"\"\n",
    "        # Define the grid size\n",
    "        self.grid_size = 4  # 4x4 grid\n",
    "        self.state_space = self.grid_size * self.grid_size  # Total number of states\n",
    "        self.action_space = 4  # Number of possible actions (UP, DOWN, LEFT, RIGHT)\n",
    "        self.current_state = random.randint(0, self.state_space - 1)  # Randomly initialize current state\n",
    "        self.goal_state = 15  # Goal state located at the bottom-right corner\n",
    "        self.done = False  # Flag to indicate if the episode is done\n",
    "        self.obstacles = [5, 6, 10, 11]  # Define obstacles in the grid\n",
    "        self.gamma = 0.95  # Discount factor for future rewards\n",
    "\n",
    "    # ==========================\n",
    "    # Reset Function\n",
    "    # ==========================\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reset the environment to a random initial state.\n",
    "        Returns the initial state after resetting.\n",
    "        \"\"\"\n",
    "        # Reset to a random state that is not an obstacle or the goal\n",
    "        self.current_state = random.choice([i for i in range(self.state_space) if i not in self.obstacles and i != self.goal_state])\n",
    "        self.done = False  # Reset the done flag\n",
    "        return self.current_state  # Return the initial state\n",
    "\n",
    "    # ==========================\n",
    "    # Step Function\n",
    "    # ==========================\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Execute the given action in the current state.\n",
    "        Update the state based on the action taken, return the next state,\n",
    "        reward received, and whether the episode is done.\n",
    "        \"\"\"\n",
    "        # Convert the current state to row and column in the grid\n",
    "        row, col = divmod(self.current_state, self.grid_size)\n",
    "\n",
    "        # Determine the new position based on the action taken\n",
    "        if action == 0:  # UP\n",
    "            new_row, new_col = max(row - 1, 0), col\n",
    "        elif action == 1:  # DOWN\n",
    "            new_row, new_col = min(row + 1, self.grid_size - 1), col\n",
    "        elif action == 2:  # LEFT\n",
    "            new_row, new_col = row, max(col - 1, 0)\n",
    "        else:  # RIGHT\n",
    "            new_row, new_col = row, min(col + 1, self.grid_size - 1)\n",
    "\n",
    "        new_state = new_row * self.grid_size + new_col  # Calculate the new state\n",
    "\n",
    "        # Check if the new state is an obstacle\n",
    "        if new_state in self.obstacles:\n",
    "            reward = -10  # Penalty for hitting an obstacle\n",
    "            next_state = self.current_state  # Stay in the same state\n",
    "        else:\n",
    "            if new_state == self.goal_state:\n",
    "                reward = 10  # Reward for reaching the goal\n",
    "                self.done = True  # Mark the episode as done\n",
    "            else:\n",
    "                reward = -1  # Penalty for a normal move\n",
    "            next_state = new_state  # Update the next state\n",
    "\n",
    "        self.current_state = next_state  # Update the current state\n",
    "        return next_state, reward, self.done  # Return next state, reward, and done status\n",
    "\n",
    "    # ==========================\n",
    "    # Render Function\n",
    "    # ==========================\n",
    "    def render(self):\n",
    "        \"\"\"\n",
    "        Display the current state of the environment.\n",
    "        \"\"\"\n",
    "        grid = np.zeros((self.grid_size, self.grid_size), dtype=int)  # Create a grid of zeros\n",
    "        for obstacle in self.obstacles:\n",
    "            grid[obstacle // self.grid_size][obstacle % self.grid_size] = -1  # Mark obstacles in the grid\n",
    "        grid[self.goal_state // self.grid_size][self.goal_state % self.grid_size] = 1  # Mark the goal\n",
    "        grid[self.current_state // self.grid_size][self.current_state % self.grid_size] = 2  # Mark the current position\n",
    "        print(grid)  # Print the grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TzqAiNWAqjlb"
   },
   "source": [
    "## Simulation Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P69lcQ2Oqhji",
    "outputId": "665fefed-f388-4cee-856c-df6b1faceb95"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2  0  0  0]\n",
      " [ 0 -1 -1  0]\n",
      " [ 0  0 -1 -1]\n",
      " [ 0  0  0  1]]\n",
      "Action Taken: 0, Reward: -1, Next State: 0\n",
      "[[ 0  0  0  0]\n",
      " [ 2 -1 -1  0]\n",
      " [ 0  0 -1 -1]\n",
      " [ 0  0  0  1]]\n",
      "Action Taken: 1, Reward: -1, Next State: 4\n",
      "[[ 2  0  0  0]\n",
      " [ 0 -1 -1  0]\n",
      " [ 0  0 -1 -1]\n",
      " [ 0  0  0  1]]\n",
      "Action Taken: 0, Reward: -1, Next State: 0\n",
      "[[ 0  0  0  0]\n",
      " [ 2 -1 -1  0]\n",
      " [ 0  0 -1 -1]\n",
      " [ 0  0  0  1]]\n",
      "Action Taken: 1, Reward: -1, Next State: 4\n",
      "[[ 2  0  0  0]\n",
      " [ 0 -1 -1  0]\n",
      " [ 0  0 -1 -1]\n",
      " [ 0  0  0  1]]\n",
      "Action Taken: 0, Reward: -1, Next State: 0\n",
      "[[ 0  0  0  0]\n",
      " [ 2 -1 -1  0]\n",
      " [ 0  0 -1 -1]\n",
      " [ 0  0  0  1]]\n",
      "Action Taken: 1, Reward: -1, Next State: 4\n",
      "[[ 2  0  0  0]\n",
      " [ 0 -1 -1  0]\n",
      " [ 0  0 -1 -1]\n",
      " [ 0  0  0  1]]\n",
      "Action Taken: 0, Reward: -1, Next State: 0\n",
      "[[ 0  2  0  0]\n",
      " [ 0 -1 -1  0]\n",
      " [ 0  0 -1 -1]\n",
      " [ 0  0  0  1]]\n",
      "Action Taken: 3, Reward: -1, Next State: 1\n",
      "[[ 0  0  2  0]\n",
      " [ 0 -1 -1  0]\n",
      " [ 0  0 -1 -1]\n",
      " [ 0  0  0  1]]\n",
      "Action Taken: 3, Reward: -1, Next State: 2\n",
      "[[ 0  0  2  0]\n",
      " [ 0 -1 -1  0]\n",
      " [ 0  0 -1 -1]\n",
      " [ 0  0  0  1]]\n",
      "Action Taken: 0, Reward: -1, Next State: 2\n",
      "[[ 0  0  2  0]\n",
      " [ 0 -1 -1  0]\n",
      " [ 0  0 -1 -1]\n",
      " [ 0  0  0  1]]\n",
      "Action Taken: 1, Reward: -10, Next State: 2\n",
      "[[ 0  0  2  0]\n",
      " [ 0 -1 -1  0]\n",
      " [ 0  0 -1 -1]\n",
      " [ 0  0  0  1]]\n",
      "Action Taken: 1, Reward: -10, Next State: 2\n",
      "[[ 0  0  2  0]\n",
      " [ 0 -1 -1  0]\n",
      " [ 0  0 -1 -1]\n",
      " [ 0  0  0  1]]\n",
      "Action Taken: 1, Reward: -10, Next State: 2\n",
      "[[ 0  0  2  0]\n",
      " [ 0 -1 -1  0]\n",
      " [ 0  0 -1 -1]\n",
      " [ 0  0  0  1]]\n",
      "Action Taken: 0, Reward: -1, Next State: 2\n",
      "[[ 0  0  2  0]\n",
      " [ 0 -1 -1  0]\n",
      " [ 0  0 -1 -1]\n",
      " [ 0  0  0  1]]\n",
      "Action Taken: 1, Reward: -10, Next State: 2\n",
      "[[ 0  0  2  0]\n",
      " [ 0 -1 -1  0]\n",
      " [ 0  0 -1 -1]\n",
      " [ 0  0  0  1]]\n",
      "Action Taken: 0, Reward: -1, Next State: 2\n",
      "[[ 0  2  0  0]\n",
      " [ 0 -1 -1  0]\n",
      " [ 0  0 -1 -1]\n",
      " [ 0  0  0  1]]\n",
      "Action Taken: 2, Reward: -1, Next State: 1\n",
      "[[ 0  2  0  0]\n",
      " [ 0 -1 -1  0]\n",
      " [ 0  0 -1 -1]\n",
      " [ 0  0  0  1]]\n",
      "Action Taken: 1, Reward: -10, Next State: 1\n",
      "[[ 0  2  0  0]\n",
      " [ 0 -1 -1  0]\n",
      " [ 0  0 -1 -1]\n",
      " [ 0  0  0  1]]\n",
      "Action Taken: 1, Reward: -10, Next State: 1\n",
      "[[ 0  2  0  0]\n",
      " [ 0 -1 -1  0]\n",
      " [ 0  0 -1 -1]\n",
      " [ 0  0  0  1]]\n",
      "Action Taken: 0, Reward: -1, Next State: 1\n",
      "[[ 0  2  0  0]\n",
      " [ 0 -1 -1  0]\n",
      " [ 0  0 -1 -1]\n",
      " [ 0  0  0  1]]\n",
      "Action Taken: 0, Reward: -1, Next State: 1\n",
      "[[ 2  0  0  0]\n",
      " [ 0 -1 -1  0]\n",
      " [ 0  0 -1 -1]\n",
      " [ 0  0  0  1]]\n",
      "Action Taken: 2, Reward: -1, Next State: 0\n",
      "[[ 0  0  0  0]\n",
      " [ 2 -1 -1  0]\n",
      " [ 0  0 -1 -1]\n",
      " [ 0  0  0  1]]\n",
      "Action Taken: 1, Reward: -1, Next State: 4\n",
      "[[ 2  0  0  0]\n",
      " [ 0 -1 -1  0]\n",
      " [ 0  0 -1 -1]\n",
      " [ 0  0  0  1]]\n",
      "Action Taken: 0, Reward: -1, Next State: 0\n",
      "[[ 2  0  0  0]\n",
      " [ 0 -1 -1  0]\n",
      " [ 0  0 -1 -1]\n",
      " [ 0  0  0  1]]\n",
      "Action Taken: 2, Reward: -1, Next State: 0\n",
      "[[ 0  0  0  0]\n",
      " [ 2 -1 -1  0]\n",
      " [ 0  0 -1 -1]\n",
      " [ 0  0  0  1]]\n",
      "Action Taken: 1, Reward: -1, Next State: 4\n",
      "[[ 0  0  0  0]\n",
      " [ 2 -1 -1  0]\n",
      " [ 0  0 -1 -1]\n",
      " [ 0  0  0  1]]\n",
      "Action Taken: 2, Reward: -1, Next State: 4\n",
      "[[ 0  0  0  0]\n",
      " [ 2 -1 -1  0]\n",
      " [ 0  0 -1 -1]\n",
      " [ 0  0  0  1]]\n",
      "Action Taken: 2, Reward: -1, Next State: 4\n",
      "[[ 0  0  0  0]\n",
      " [ 0 -1 -1  0]\n",
      " [ 2  0 -1 -1]\n",
      " [ 0  0  0  1]]\n",
      "Action Taken: 1, Reward: -1, Next State: 8\n",
      "[[ 0  0  0  0]\n",
      " [ 2 -1 -1  0]\n",
      " [ 0  0 -1 -1]\n",
      " [ 0  0  0  1]]\n",
      "Action Taken: 0, Reward: -1, Next State: 4\n",
      "[[ 0  0  0  0]\n",
      " [ 0 -1 -1  0]\n",
      " [ 2  0 -1 -1]\n",
      " [ 0  0  0  1]]\n",
      "Action Taken: 1, Reward: -1, Next State: 8\n",
      "[[ 0  0  0  0]\n",
      " [ 0 -1 -1  0]\n",
      " [ 0  0 -1 -1]\n",
      " [ 2  0  0  1]]\n",
      "Action Taken: 1, Reward: -1, Next State: 12\n",
      "[[ 0  0  0  0]\n",
      " [ 0 -1 -1  0]\n",
      " [ 0  0 -1 -1]\n",
      " [ 2  0  0  1]]\n",
      "Action Taken: 2, Reward: -1, Next State: 12\n",
      "[[ 0  0  0  0]\n",
      " [ 0 -1 -1  0]\n",
      " [ 0  0 -1 -1]\n",
      " [ 2  0  0  1]]\n",
      "Action Taken: 2, Reward: -1, Next State: 12\n",
      "[[ 0  0  0  0]\n",
      " [ 0 -1 -1  0]\n",
      " [ 0  0 -1 -1]\n",
      " [ 2  0  0  1]]\n",
      "Action Taken: 2, Reward: -1, Next State: 12\n",
      "[[ 0  0  0  0]\n",
      " [ 0 -1 -1  0]\n",
      " [ 2  0 -1 -1]\n",
      " [ 0  0  0  1]]\n",
      "Action Taken: 0, Reward: -1, Next State: 8\n",
      "[[ 0  0  0  0]\n",
      " [ 0 -1 -1  0]\n",
      " [ 0  0 -1 -1]\n",
      " [ 2  0  0  1]]\n",
      "Action Taken: 1, Reward: -1, Next State: 12\n",
      "[[ 0  0  0  0]\n",
      " [ 0 -1 -1  0]\n",
      " [ 2  0 -1 -1]\n",
      " [ 0  0  0  1]]\n",
      "Action Taken: 0, Reward: -1, Next State: 8\n",
      "[[ 0  0  0  0]\n",
      " [ 0 -1 -1  0]\n",
      " [ 0  2 -1 -1]\n",
      " [ 0  0  0  1]]\n",
      "Action Taken: 3, Reward: -1, Next State: 9\n",
      "[[ 0  0  0  0]\n",
      " [ 0 -1 -1  0]\n",
      " [ 0  2 -1 -1]\n",
      " [ 0  0  0  1]]\n",
      "Action Taken: 3, Reward: -10, Next State: 9\n",
      "[[ 0  0  0  0]\n",
      " [ 0 -1 -1  0]\n",
      " [ 2  0 -1 -1]\n",
      " [ 0  0  0  1]]\n",
      "Action Taken: 2, Reward: -1, Next State: 8\n",
      "[[ 0  0  0  0]\n",
      " [ 2 -1 -1  0]\n",
      " [ 0  0 -1 -1]\n",
      " [ 0  0  0  1]]\n",
      "Action Taken: 0, Reward: -1, Next State: 4\n",
      "[[ 2  0  0  0]\n",
      " [ 0 -1 -1  0]\n",
      " [ 0  0 -1 -1]\n",
      " [ 0  0  0  1]]\n",
      "Action Taken: 0, Reward: -1, Next State: 0\n",
      "[[ 2  0  0  0]\n",
      " [ 0 -1 -1  0]\n",
      " [ 0  0 -1 -1]\n",
      " [ 0  0  0  1]]\n",
      "Action Taken: 2, Reward: -1, Next State: 0\n",
      "[[ 0  2  0  0]\n",
      " [ 0 -1 -1  0]\n",
      " [ 0  0 -1 -1]\n",
      " [ 0  0  0  1]]\n",
      "Action Taken: 3, Reward: -1, Next State: 1\n",
      "[[ 0  2  0  0]\n",
      " [ 0 -1 -1  0]\n",
      " [ 0  0 -1 -1]\n",
      " [ 0  0  0  1]]\n",
      "Action Taken: 1, Reward: -10, Next State: 1\n",
      "[[ 2  0  0  0]\n",
      " [ 0 -1 -1  0]\n",
      " [ 0  0 -1 -1]\n",
      " [ 0  0  0  1]]\n",
      "Action Taken: 2, Reward: -1, Next State: 0\n",
      "[[ 0  2  0  0]\n",
      " [ 0 -1 -1  0]\n",
      " [ 0  0 -1 -1]\n",
      " [ 0  0  0  1]]\n",
      "Action Taken: 3, Reward: -1, Next State: 1\n",
      "[[ 0  0  2  0]\n",
      " [ 0 -1 -1  0]\n",
      " [ 0  0 -1 -1]\n",
      " [ 0  0  0  1]]\n",
      "Action Taken: 3, Reward: -1, Next State: 2\n",
      "[[ 0  0  0  2]\n",
      " [ 0 -1 -1  0]\n",
      " [ 0  0 -1 -1]\n",
      " [ 0  0  0  1]]\n",
      "Action Taken: 3, Reward: -1, Next State: 3\n",
      "[[ 0  0  0  2]\n",
      " [ 0 -1 -1  0]\n",
      " [ 0  0 -1 -1]\n",
      " [ 0  0  0  1]]\n",
      "Action Taken: 3, Reward: -1, Next State: 3\n",
      "[[ 0  0  0  0]\n",
      " [ 0 -1 -1  2]\n",
      " [ 0  0 -1 -1]\n",
      " [ 0  0  0  1]]\n",
      "Action Taken: 1, Reward: -1, Next State: 7\n",
      "[[ 0  0  0  0]\n",
      " [ 0 -1 -1  2]\n",
      " [ 0  0 -1 -1]\n",
      " [ 0  0  0  1]]\n",
      "Action Taken: 3, Reward: -1, Next State: 7\n",
      "[[ 0  0  0  0]\n",
      " [ 0 -1 -1  2]\n",
      " [ 0  0 -1 -1]\n",
      " [ 0  0  0  1]]\n",
      "Action Taken: 1, Reward: -10, Next State: 7\n",
      "[[ 0  0  0  0]\n",
      " [ 0 -1 -1  2]\n",
      " [ 0  0 -1 -1]\n",
      " [ 0  0  0  1]]\n",
      "Action Taken: 1, Reward: -10, Next State: 7\n",
      "[[ 0  0  0  0]\n",
      " [ 0 -1 -1  2]\n",
      " [ 0  0 -1 -1]\n",
      " [ 0  0  0  1]]\n",
      "Action Taken: 2, Reward: -10, Next State: 7\n",
      "[[ 0  0  0  0]\n",
      " [ 0 -1 -1  2]\n",
      " [ 0  0 -1 -1]\n",
      " [ 0  0  0  1]]\n",
      "Action Taken: 3, Reward: -1, Next State: 7\n",
      "[[ 0  0  0  0]\n",
      " [ 0 -1 -1  2]\n",
      " [ 0  0 -1 -1]\n",
      " [ 0  0  0  1]]\n",
      "Action Taken: 1, Reward: -10, Next State: 7\n",
      "[[ 0  0  0  2]\n",
      " [ 0 -1 -1  0]\n",
      " [ 0  0 -1 -1]\n",
      " [ 0  0  0  1]]\n",
      "Action Taken: 0, Reward: -1, Next State: 3\n",
      "[[ 0  0  2  0]\n",
      " [ 0 -1 -1  0]\n",
      " [ 0  0 -1 -1]\n",
      " [ 0  0  0  1]]\n",
      "Action Taken: 2, Reward: -1, Next State: 2\n",
      "[[ 0  0  0  2]\n",
      " [ 0 -1 -1  0]\n",
      " [ 0  0 -1 -1]\n",
      " [ 0  0  0  1]]\n",
      "Action Taken: 3, Reward: -1, Next State: 3\n",
      "[[ 0  0  0  2]\n",
      " [ 0 -1 -1  0]\n",
      " [ 0  0 -1 -1]\n",
      " [ 0  0  0  1]]\n",
      "Action Taken: 0, Reward: -1, Next State: 3\n",
      "[[ 0  0  0  2]\n",
      " [ 0 -1 -1  0]\n",
      " [ 0  0 -1 -1]\n",
      " [ 0  0  0  1]]\n",
      "Action Taken: 0, Reward: -1, Next State: 3\n",
      "[[ 0  0  2  0]\n",
      " [ 0 -1 -1  0]\n",
      " [ 0  0 -1 -1]\n",
      " [ 0  0  0  1]]\n",
      "Action Taken: 2, Reward: -1, Next State: 2\n",
      "[[ 0  2  0  0]\n",
      " [ 0 -1 -1  0]\n",
      " [ 0  0 -1 -1]\n",
      " [ 0  0  0  1]]\n",
      "Action Taken: 2, Reward: -1, Next State: 1\n",
      "[[ 0  2  0  0]\n",
      " [ 0 -1 -1  0]\n",
      " [ 0  0 -1 -1]\n",
      " [ 0  0  0  1]]\n",
      "Action Taken: 0, Reward: -1, Next State: 1\n",
      "[[ 2  0  0  0]\n",
      " [ 0 -1 -1  0]\n",
      " [ 0  0 -1 -1]\n",
      " [ 0  0  0  1]]\n",
      "Action Taken: 2, Reward: -1, Next State: 0\n",
      "[[ 0  2  0  0]\n",
      " [ 0 -1 -1  0]\n",
      " [ 0  0 -1 -1]\n",
      " [ 0  0  0  1]]\n",
      "Action Taken: 3, Reward: -1, Next State: 1\n",
      "[[ 0  0  2  0]\n",
      " [ 0 -1 -1  0]\n",
      " [ 0  0 -1 -1]\n",
      " [ 0  0  0  1]]\n",
      "Action Taken: 3, Reward: -1, Next State: 2\n",
      "[[ 0  0  0  2]\n",
      " [ 0 -1 -1  0]\n",
      " [ 0  0 -1 -1]\n",
      " [ 0  0  0  1]]\n",
      "Action Taken: 3, Reward: -1, Next State: 3\n",
      "[[ 0  0  0  2]\n",
      " [ 0 -1 -1  0]\n",
      " [ 0  0 -1 -1]\n",
      " [ 0  0  0  1]]\n",
      "Action Taken: 0, Reward: -1, Next State: 3\n",
      "[[ 0  0  2  0]\n",
      " [ 0 -1 -1  0]\n",
      " [ 0  0 -1 -1]\n",
      " [ 0  0  0  1]]\n",
      "Action Taken: 2, Reward: -1, Next State: 2\n",
      "[[ 0  2  0  0]\n",
      " [ 0 -1 -1  0]\n",
      " [ 0  0 -1 -1]\n",
      " [ 0  0  0  1]]\n",
      "Action Taken: 2, Reward: -1, Next State: 1\n",
      "[[ 2  0  0  0]\n",
      " [ 0 -1 -1  0]\n",
      " [ 0  0 -1 -1]\n",
      " [ 0  0  0  1]]\n",
      "Action Taken: 2, Reward: -1, Next State: 0\n",
      "[[ 0  2  0  0]\n",
      " [ 0 -1 -1  0]\n",
      " [ 0  0 -1 -1]\n",
      " [ 0  0  0  1]]\n",
      "Action Taken: 3, Reward: -1, Next State: 1\n",
      "[[ 0  2  0  0]\n",
      " [ 0 -1 -1  0]\n",
      " [ 0  0 -1 -1]\n",
      " [ 0  0  0  1]]\n",
      "Action Taken: 1, Reward: -10, Next State: 1\n",
      "[[ 2  0  0  0]\n",
      " [ 0 -1 -1  0]\n",
      " [ 0  0 -1 -1]\n",
      " [ 0  0  0  1]]\n",
      "Action Taken: 2, Reward: -1, Next State: 0\n",
      "[[ 2  0  0  0]\n",
      " [ 0 -1 -1  0]\n",
      " [ 0  0 -1 -1]\n",
      " [ 0  0  0  1]]\n",
      "Action Taken: 2, Reward: -1, Next State: 0\n",
      "[[ 2  0  0  0]\n",
      " [ 0 -1 -1  0]\n",
      " [ 0  0 -1 -1]\n",
      " [ 0  0  0  1]]\n",
      "Action Taken: 0, Reward: -1, Next State: 0\n",
      "[[ 2  0  0  0]\n",
      " [ 0 -1 -1  0]\n",
      " [ 0  0 -1 -1]\n",
      " [ 0  0  0  1]]\n",
      "Action Taken: 0, Reward: -1, Next State: 0\n",
      "[[ 0  2  0  0]\n",
      " [ 0 -1 -1  0]\n",
      " [ 0  0 -1 -1]\n",
      " [ 0  0  0  1]]\n",
      "Action Taken: 3, Reward: -1, Next State: 1\n",
      "[[ 0  0  2  0]\n",
      " [ 0 -1 -1  0]\n",
      " [ 0  0 -1 -1]\n",
      " [ 0  0  0  1]]\n",
      "Action Taken: 3, Reward: -1, Next State: 2\n",
      "[[ 0  2  0  0]\n",
      " [ 0 -1 -1  0]\n",
      " [ 0  0 -1 -1]\n",
      " [ 0  0  0  1]]\n",
      "Action Taken: 2, Reward: -1, Next State: 1\n",
      "[[ 0  2  0  0]\n",
      " [ 0 -1 -1  0]\n",
      " [ 0  0 -1 -1]\n",
      " [ 0  0  0  1]]\n",
      "Action Taken: 0, Reward: -1, Next State: 1\n",
      "[[ 0  2  0  0]\n",
      " [ 0 -1 -1  0]\n",
      " [ 0  0 -1 -1]\n",
      " [ 0  0  0  1]]\n",
      "Action Taken: 1, Reward: -10, Next State: 1\n",
      "[[ 2  0  0  0]\n",
      " [ 0 -1 -1  0]\n",
      " [ 0  0 -1 -1]\n",
      " [ 0  0  0  1]]\n",
      "Action Taken: 2, Reward: -1, Next State: 0\n",
      "[[ 2  0  0  0]\n",
      " [ 0 -1 -1  0]\n",
      " [ 0  0 -1 -1]\n",
      " [ 0  0  0  1]]\n",
      "Action Taken: 0, Reward: -1, Next State: 0\n",
      "[[ 0  2  0  0]\n",
      " [ 0 -1 -1  0]\n",
      " [ 0  0 -1 -1]\n",
      " [ 0  0  0  1]]\n",
      "Action Taken: 3, Reward: -1, Next State: 1\n",
      "[[ 0  0  2  0]\n",
      " [ 0 -1 -1  0]\n",
      " [ 0  0 -1 -1]\n",
      " [ 0  0  0  1]]\n",
      "Action Taken: 3, Reward: -1, Next State: 2\n",
      "[[ 0  0  2  0]\n",
      " [ 0 -1 -1  0]\n",
      " [ 0  0 -1 -1]\n",
      " [ 0  0  0  1]]\n",
      "Action Taken: 0, Reward: -1, Next State: 2\n",
      "[[ 0  0  0  2]\n",
      " [ 0 -1 -1  0]\n",
      " [ 0  0 -1 -1]\n",
      " [ 0  0  0  1]]\n",
      "Action Taken: 3, Reward: -1, Next State: 3\n",
      "[[ 0  0  2  0]\n",
      " [ 0 -1 -1  0]\n",
      " [ 0  0 -1 -1]\n",
      " [ 0  0  0  1]]\n",
      "Action Taken: 2, Reward: -1, Next State: 2\n",
      "[[ 0  2  0  0]\n",
      " [ 0 -1 -1  0]\n",
      " [ 0  0 -1 -1]\n",
      " [ 0  0  0  1]]\n",
      "Action Taken: 2, Reward: -1, Next State: 1\n",
      "[[ 0  0  2  0]\n",
      " [ 0 -1 -1  0]\n",
      " [ 0  0 -1 -1]\n",
      " [ 0  0  0  1]]\n",
      "Action Taken: 3, Reward: -1, Next State: 2\n",
      "[[ 0  2  0  0]\n",
      " [ 0 -1 -1  0]\n",
      " [ 0  0 -1 -1]\n",
      " [ 0  0  0  1]]\n",
      "Action Taken: 2, Reward: -1, Next State: 1\n",
      "[[ 0  2  0  0]\n",
      " [ 0 -1 -1  0]\n",
      " [ 0  0 -1 -1]\n",
      " [ 0  0  0  1]]\n",
      "Action Taken: 0, Reward: -1, Next State: 1\n",
      "[[ 0  2  0  0]\n",
      " [ 0 -1 -1  0]\n",
      " [ 0  0 -1 -1]\n",
      " [ 0  0  0  1]]\n",
      "Action Taken: 0, Reward: -1, Next State: 1\n",
      "[[ 0  0  2  0]\n",
      " [ 0 -1 -1  0]\n",
      " [ 0  0 -1 -1]\n",
      " [ 0  0  0  1]]\n",
      "Action Taken: 3, Reward: -1, Next State: 2\n",
      "[[ 0  0  2  0]\n",
      " [ 0 -1 -1  0]\n",
      " [ 0  0 -1 -1]\n",
      " [ 0  0  0  1]]\n",
      "Action Taken: 0, Reward: -1, Next State: 2\n",
      "[[ 0  0  0  2]\n",
      " [ 0 -1 -1  0]\n",
      " [ 0  0 -1 -1]\n",
      " [ 0  0  0  1]]\n",
      "Action Taken: 3, Reward: -1, Next State: 3\n",
      "[[ 0  0  0  2]\n",
      " [ 0 -1 -1  0]\n",
      " [ 0  0 -1 -1]\n",
      " [ 0  0  0  1]]\n",
      "Action Taken: 0, Reward: -1, Next State: 3\n",
      "[[ 0  0  2  0]\n",
      " [ 0 -1 -1  0]\n",
      " [ 0  0 -1 -1]\n",
      " [ 0  0  0  1]]\n",
      "Action Taken: 2, Reward: -1, Next State: 2\n",
      "[[ 0  0  2  0]\n",
      " [ 0 -1 -1  0]\n",
      " [ 0  0 -1 -1]\n",
      " [ 0  0  0  1]]\n",
      "Action Taken: 0, Reward: -1, Next State: 2\n",
      "[[ 0  0  2  0]\n",
      " [ 0 -1 -1  0]\n",
      " [ 0  0 -1 -1]\n",
      " [ 0  0  0  1]]\n",
      "Action Taken: 0, Reward: -1, Next State: 2\n",
      "[[ 0  2  0  0]\n",
      " [ 0 -1 -1  0]\n",
      " [ 0  0 -1 -1]\n",
      " [ 0  0  0  1]]\n",
      "Action Taken: 2, Reward: -1, Next State: 1\n",
      "[[ 0  2  0  0]\n",
      " [ 0 -1 -1  0]\n",
      " [ 0  0 -1 -1]\n",
      " [ 0  0  0  1]]\n",
      "Action Taken: 0, Reward: -1, Next State: 1\n",
      "[[ 2  0  0  0]\n",
      " [ 0 -1 -1  0]\n",
      " [ 0  0 -1 -1]\n",
      " [ 0  0  0  1]]\n",
      "Action Taken: 2, Reward: -1, Next State: 0\n",
      "[[ 0  0  0  0]\n",
      " [ 2 -1 -1  0]\n",
      " [ 0  0 -1 -1]\n",
      " [ 0  0  0  1]]\n",
      "Action Taken: 1, Reward: -1, Next State: 4\n",
      "[[ 2  0  0  0]\n",
      " [ 0 -1 -1  0]\n",
      " [ 0  0 -1 -1]\n",
      " [ 0  0  0  1]]\n",
      "Action Taken: 0, Reward: -1, Next State: 0\n",
      "[[ 2  0  0  0]\n",
      " [ 0 -1 -1  0]\n",
      " [ 0  0 -1 -1]\n",
      " [ 0  0  0  1]]\n",
      "Action Taken: 0, Reward: -1, Next State: 0\n",
      "[[ 2  0  0  0]\n",
      " [ 0 -1 -1  0]\n",
      " [ 0  0 -1 -1]\n",
      " [ 0  0  0  1]]\n",
      "Action Taken: 0, Reward: -1, Next State: 0\n",
      "[[ 0  0  0  0]\n",
      " [ 2 -1 -1  0]\n",
      " [ 0  0 -1 -1]\n",
      " [ 0  0  0  1]]\n",
      "Action Taken: 1, Reward: -1, Next State: 4\n",
      "[[ 0  0  0  0]\n",
      " [ 2 -1 -1  0]\n",
      " [ 0  0 -1 -1]\n",
      " [ 0  0  0  1]]\n",
      "Action Taken: 2, Reward: -1, Next State: 4\n",
      "[[ 0  0  0  0]\n",
      " [ 2 -1 -1  0]\n",
      " [ 0  0 -1 -1]\n",
      " [ 0  0  0  1]]\n",
      "Action Taken: 2, Reward: -1, Next State: 4\n",
      "[[ 0  0  0  0]\n",
      " [ 2 -1 -1  0]\n",
      " [ 0  0 -1 -1]\n",
      " [ 0  0  0  1]]\n",
      "Action Taken: 2, Reward: -1, Next State: 4\n",
      "[[ 0  0  0  0]\n",
      " [ 0 -1 -1  0]\n",
      " [ 2  0 -1 -1]\n",
      " [ 0  0  0  1]]\n",
      "Action Taken: 1, Reward: -1, Next State: 8\n",
      "[[ 0  0  0  0]\n",
      " [ 0 -1 -1  0]\n",
      " [ 2  0 -1 -1]\n",
      " [ 0  0  0  1]]\n",
      "Action Taken: 2, Reward: -1, Next State: 8\n",
      "[[ 0  0  0  0]\n",
      " [ 0 -1 -1  0]\n",
      " [ 2  0 -1 -1]\n",
      " [ 0  0  0  1]]\n",
      "Action Taken: 2, Reward: -1, Next State: 8\n",
      "[[ 0  0  0  0]\n",
      " [ 2 -1 -1  0]\n",
      " [ 0  0 -1 -1]\n",
      " [ 0  0  0  1]]\n",
      "Action Taken: 0, Reward: -1, Next State: 4\n",
      "[[ 0  0  0  0]\n",
      " [ 0 -1 -1  0]\n",
      " [ 2  0 -1 -1]\n",
      " [ 0  0  0  1]]\n",
      "Action Taken: 1, Reward: -1, Next State: 8\n",
      "[[ 0  0  0  0]\n",
      " [ 0 -1 -1  0]\n",
      " [ 0  0 -1 -1]\n",
      " [ 2  0  0  1]]\n",
      "Action Taken: 1, Reward: -1, Next State: 12\n",
      "[[ 0  0  0  0]\n",
      " [ 0 -1 -1  0]\n",
      " [ 2  0 -1 -1]\n",
      " [ 0  0  0  1]]\n",
      "Action Taken: 0, Reward: -1, Next State: 8\n",
      "[[ 0  0  0  0]\n",
      " [ 2 -1 -1  0]\n",
      " [ 0  0 -1 -1]\n",
      " [ 0  0  0  1]]\n",
      "Action Taken: 0, Reward: -1, Next State: 4\n",
      "[[ 0  0  0  0]\n",
      " [ 0 -1 -1  0]\n",
      " [ 2  0 -1 -1]\n",
      " [ 0  0  0  1]]\n",
      "Action Taken: 1, Reward: -1, Next State: 8\n",
      "[[ 0  0  0  0]\n",
      " [ 2 -1 -1  0]\n",
      " [ 0  0 -1 -1]\n",
      " [ 0  0  0  1]]\n",
      "Action Taken: 0, Reward: -1, Next State: 4\n",
      "[[ 2  0  0  0]\n",
      " [ 0 -1 -1  0]\n",
      " [ 0  0 -1 -1]\n",
      " [ 0  0  0  1]]\n",
      "Action Taken: 0, Reward: -1, Next State: 0\n",
      "[[ 2  0  0  0]\n",
      " [ 0 -1 -1  0]\n",
      " [ 0  0 -1 -1]\n",
      " [ 0  0  0  1]]\n",
      "Action Taken: 2, Reward: -1, Next State: 0\n",
      "[[ 0  2  0  0]\n",
      " [ 0 -1 -1  0]\n",
      " [ 0  0 -1 -1]\n",
      " [ 0  0  0  1]]\n",
      "Action Taken: 3, Reward: -1, Next State: 1\n",
      "[[ 0  2  0  0]\n",
      " [ 0 -1 -1  0]\n",
      " [ 0  0 -1 -1]\n",
      " [ 0  0  0  1]]\n",
      "Action Taken: 1, Reward: -10, Next State: 1\n",
      "[[ 0  2  0  0]\n",
      " [ 0 -1 -1  0]\n",
      " [ 0  0 -1 -1]\n",
      " [ 0  0  0  1]]\n",
      "Action Taken: 1, Reward: -10, Next State: 1\n",
      "[[ 2  0  0  0]\n",
      " [ 0 -1 -1  0]\n",
      " [ 0  0 -1 -1]\n",
      " [ 0  0  0  1]]\n",
      "Action Taken: 2, Reward: -1, Next State: 0\n",
      "[[ 0  0  0  0]\n",
      " [ 2 -1 -1  0]\n",
      " [ 0  0 -1 -1]\n",
      " [ 0  0  0  1]]\n",
      "Action Taken: 1, Reward: -1, Next State: 4\n",
      "[[ 2  0  0  0]\n",
      " [ 0 -1 -1  0]\n",
      " [ 0  0 -1 -1]\n",
      " [ 0  0  0  1]]\n",
      "Action Taken: 0, Reward: -1, Next State: 0\n",
      "[[ 2  0  0  0]\n",
      " [ 0 -1 -1  0]\n",
      " [ 0  0 -1 -1]\n",
      " [ 0  0  0  1]]\n",
      "Action Taken: 2, Reward: -1, Next State: 0\n",
      "[[ 0  2  0  0]\n",
      " [ 0 -1 -1  0]\n",
      " [ 0  0 -1 -1]\n",
      " [ 0  0  0  1]]\n",
      "Action Taken: 3, Reward: -1, Next State: 1\n",
      "[[ 2  0  0  0]\n",
      " [ 0 -1 -1  0]\n",
      " [ 0  0 -1 -1]\n",
      " [ 0  0  0  1]]\n",
      "Action Taken: 2, Reward: -1, Next State: 0\n",
      "[[ 2  0  0  0]\n",
      " [ 0 -1 -1  0]\n",
      " [ 0  0 -1 -1]\n",
      " [ 0  0  0  1]]\n",
      "Action Taken: 0, Reward: -1, Next State: 0\n",
      "[[ 0  0  0  0]\n",
      " [ 2 -1 -1  0]\n",
      " [ 0  0 -1 -1]\n",
      " [ 0  0  0  1]]\n",
      "Action Taken: 1, Reward: -1, Next State: 4\n",
      "[[ 0  0  0  0]\n",
      " [ 0 -1 -1  0]\n",
      " [ 2  0 -1 -1]\n",
      " [ 0  0  0  1]]\n",
      "Action Taken: 1, Reward: -1, Next State: 8\n",
      "[[ 0  0  0  0]\n",
      " [ 0 -1 -1  0]\n",
      " [ 2  0 -1 -1]\n",
      " [ 0  0  0  1]]\n",
      "Action Taken: 2, Reward: -1, Next State: 8\n",
      "[[ 0  0  0  0]\n",
      " [ 0 -1 -1  0]\n",
      " [ 0  0 -1 -1]\n",
      " [ 2  0  0  1]]\n",
      "Action Taken: 1, Reward: -1, Next State: 12\n",
      "[[ 0  0  0  0]\n",
      " [ 0 -1 -1  0]\n",
      " [ 0  0 -1 -1]\n",
      " [ 2  0  0  1]]\n",
      "Action Taken: 2, Reward: -1, Next State: 12\n",
      "[[ 0  0  0  0]\n",
      " [ 0 -1 -1  0]\n",
      " [ 0  0 -1 -1]\n",
      " [ 2  0  0  1]]\n",
      "Action Taken: 2, Reward: -1, Next State: 12\n",
      "[[ 0  0  0  0]\n",
      " [ 0 -1 -1  0]\n",
      " [ 0  0 -1 -1]\n",
      " [ 0  2  0  1]]\n",
      "Action Taken: 3, Reward: -1, Next State: 13\n",
      "[[ 0  0  0  0]\n",
      " [ 0 -1 -1  0]\n",
      " [ 0  0 -1 -1]\n",
      " [ 2  0  0  1]]\n",
      "Action Taken: 2, Reward: -1, Next State: 12\n",
      "[[ 0  0  0  0]\n",
      " [ 0 -1 -1  0]\n",
      " [ 0  0 -1 -1]\n",
      " [ 0  2  0  1]]\n",
      "Action Taken: 3, Reward: -1, Next State: 13\n",
      "[[ 0  0  0  0]\n",
      " [ 0 -1 -1  0]\n",
      " [ 0  0 -1 -1]\n",
      " [ 0  0  2  1]]\n",
      "Action Taken: 3, Reward: -1, Next State: 14\n",
      "[[ 0  0  0  0]\n",
      " [ 0 -1 -1  0]\n",
      " [ 0  0 -1 -1]\n",
      " [ 0  0  0  2]]\n",
      "Action Taken: 3, Reward: 10, Next State: 15\n",
      "Episode finished.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Create an instance of the GridWorld environment\n",
    "    env = GridWorldEnv()\n",
    "    # Reset the environment to get the initial state\n",
    "    state = env.reset()\n",
    "    done = False  # Initialize done flag\n",
    "\n",
    "    # Run the simulation\n",
    "    while not done:\n",
    "        # Randomly choose an action for simplicity\n",
    "        action = random.randint(0, 3)  # Select a random action from available actions\n",
    "        # Execute the action and observe the next state and reward\n",
    "        next_state, reward, done = env.step(action)  # Take a step in the environment\n",
    "        # Render the current state of the environment\n",
    "        env.render()  # Show the current state\n",
    "        # Print the action taken, reward received, and next state\n",
    "        print(f\"Action Taken: {action}, Reward: {reward}, Next State: {next_state}\")  # Output the action, reward, and next state\n",
    "\n",
    "    print(\"Episode finished.\")  # Indicate that the episode has ended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
